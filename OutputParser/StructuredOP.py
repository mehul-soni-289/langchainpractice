from langchain_huggingface import ChatHuggingFace , HuggingFaceEndpoint 
from langchain_core.prompts import PromptTemplate
from langchain.output_parsers import StructuredOutputParser,ResponseSchema
from dotenv import load_dotenv 

load_dotenv() 

llm = HuggingFaceEndpoint (
    repo_id='meta-llama/Meta-Llama-3-8B-Instruct' , 
    task = 'text-generation'
)

model = ChatHuggingFace(llm=llm)

schema = [
ResponseSchema(name='fact_1' , description='Fact 1 about description') , 
ResponseSchema(name='fact_2' , description='Fact 2 about description') , 
ResponseSchema(name='fact_3' , description='Fact 3 about description') , 
]

parser = StructuredOutputParser.from_response_schemas(schema)

template   = PromptTemplate(
    template='Give 3 facts abbout the topic {topic} \n {format_instruction}' , 
    input_variables=['topic'] , 
    partial_variables={'format_instruction' : parser.get_format_instructions()}


)

chain = template |model | parser 

result = chain.invoke({'topic' : 'black hole'})

print(result)